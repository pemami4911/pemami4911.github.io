{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The PyTorch to ONNX Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some standard imports\n",
    "import torch\n",
    "import torch.onnx\n",
    "import onnx\n",
    "import onnxruntime\n",
    "import torchvision.models as models\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.mobilenet_v2(pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "# Input to the model\n",
    "x = torch.randn(batch_size, 3, 224, 224, requires_grad=True)\n",
    "torch_out = model(x)\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(model,                     # model being run\n",
    "                  x,                         # model input (or a tuple for multiple inputs)\n",
    "                  \"mobilenet_v2.onnx\",      # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=10,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output'], # the model's output names\n",
    "                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "                                'output' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.load(\"mobilenet_v2.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_session = onnxruntime.InferenceSession(\"mobilenet_v2.onnx\")\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "# compute ONNX Runtime output prediction\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "# compare ONNX Runtime and PyTorch results\n",
    "np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)\n",
    "\n",
    "print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the MobileNetV2 IR on the NCS2\n",
    "\n",
    "The following code is adapted from one of the provided OpenVINO image classification demos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " Copyright (c) 2018-2021 Intel Corporation\n",
    "\n",
    " Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    " you may not use this file except in compliance with the License.\n",
    " You may obtain a copy of the License at\n",
    "\n",
    "      http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    " Unless required by applicable law or agreed to in writing, software\n",
    " distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    " WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    " See the License for the specific language governing permissions and\n",
    " limitations under the License.\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import logging as log\n",
    "from openvino.inference_engine import IECore\n",
    "import ngraph as ng\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"mobilenet_v2.xml\"\n",
    "device = 'MYRIAD'\n",
    "log.basicConfig(format=\"[ %(levelname)s ] %(message)s\", level=log.INFO, stream=sys.stdout)\n",
    "log.info(\"Loading Inference Engine\")\n",
    "ie = IECore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---1. Read a model in OpenVINO Intermediate Representation (.xml and .bin files) or ONNX (.onnx file) format ---\n",
    "log.info(f\"Loading network:\\n\\t{model}\")\n",
    "net = ie.read_network(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# ------------- 2. Load Plugin for inference engine and extensions library if specified --------------\n",
    "log.info(\"Device info:\")\n",
    "versions = ie.get_versions(device)\n",
    "print(f\"{' ' * 8}{device}\")\n",
    "print(f\"{' ' * 8}MKLDNNPlugin version ......... {versions[device].major}.{versions[device].minor}\")\n",
    "print(f\"{' ' * 8}Build ........... {versions[device].build_number}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# --------------------------- 3. Read and preprocess input --------------------------------------------\n",
    "input = 'car.png'\n",
    "for input_key in net.input_info:\n",
    "    if len(net.input_info[input_key].input_data.layout) == 4:\n",
    "        n, c, h, w = net.input_info[input_key].input_data.shape\n",
    "        \n",
    "images = np.ndarray(shape=(n, c, h, w))\n",
    "images_hw = []\n",
    "for i in range(n):\n",
    "    image = cv2.imread(input)\n",
    "    ih, iw = image.shape[:-1]\n",
    "    images_hw.append((ih, iw))\n",
    "    log.info(\"File was added: \")\n",
    "    log.info(f\"        {input}\")\n",
    "    if (ih, iw) != (h, w):\n",
    "        log.warning(f\"Image {input} is resized from {image.shape[:-1]} to {(h, w)}\")\n",
    "        image = cv2.resize(image, (w, h))\n",
    "    # BGR to RGB\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # Change data layout from HWC to CHW \n",
    "    image = image.transpose((2, 0, 1))  \n",
    "     # Change to (0,1)\n",
    "    image = image / 255. \n",
    "    # Subtract mean\n",
    "    image -= np.array([0.485, 0.456, 0.406]).reshape(3,1,1)\n",
    "    image /= np.array([0.229, 0.224, 0.225]).reshape(3,1,1)\n",
    "    images[i] = image        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# --------------------------- 4. Configure input & output ---------------------------------------------\n",
    "# --------------------------- Prepare input blobs -----------------------------------------------------\n",
    "log.info(\"Preparing input blobs\")\n",
    "assert (len(net.input_info.keys()) == 1 or len(\n",
    "    net.input_info.keys()) == 2), \"Sample supports topologies only with 1 or 2 inputs\"\n",
    "out_blob = next(iter(net.outputs))\n",
    "for input_key in net.input_info:\n",
    "    input_name = input_key\n",
    "    net.input_info[input_key].precision = 'FP32'\n",
    "    break\n",
    "\n",
    "data = {}\n",
    "data[input_name] = images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# --------------------------- Performing inference ----------------------------------------------------\n",
    "log.info(\"Loading model to the device\")\n",
    "exec_net = ie.load_network(network=net, device_name=device)\n",
    "log.info(\"Creating infer request and starting inference\")\n",
    "res = exec_net.infer(inputs=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info(\"Processing output blob\")\n",
    "log.info(f\"Top 10 results: \")\n",
    "labels_map = []\n",
    "with open('mobilenet_v2.labels', 'r') as f:\n",
    "    for line in f:\n",
    "        labels_map += [line.strip('\\n')]\n",
    "classid_str = \"label\"\n",
    "probability_str = \"probability\"\n",
    "for i, probs in enumerate(res[out_blob]):\n",
    "    probs = np.squeeze(probs)\n",
    "    top_ind = np.argsort(probs)[-10:][::-1]\n",
    "    print(f\"Image {input}\\n\")\n",
    "    print(classid_str, probability_str)\n",
    "    print(f\"{'-' * len(classid_str)} {'-' * len(probability_str)}\")\n",
    "    for id in top_ind:\n",
    "        det_label = labels_map[id] if labels_map else f\"{id}\"\n",
    "        label_length = len(det_label)\n",
    "        space_num_before = (7 - label_length) // 2\n",
    "        space_num_after = 7 - (space_num_before + label_length) + 2\n",
    "        print(f\"{det_label} ... {probs[id]:.7f} %\")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ae2f3d7d6ab363ba7037e0c4cde2f3357ecb2b15569607bf4ea9d66b32bde94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
