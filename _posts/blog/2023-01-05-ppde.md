---
layout: post
title: "Plug & Play Directed Evolution of Proteins with Gradient-based Discrete MCMC"
date: 2023-01-05
category: blog
byline: "Web page for PPDE"
---

<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } },
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>

---
[**Patrick Emami**](https://pemami4911.github.io), [Aidan Perreault](https://dblp.org/pid/230/7966.html), [Jeff Law](https://scholar.google.com/citations?user=N4r7gmAAAAAJ&hl=en&oi=ao), [David Biagioni](https://scholar.google.com/citations?user=971GyxQAAAAJ&hl=en&oi=sra), [Peter C. St. John](https://pcstj.com/cv/). *Under review.*  
Presented at [NeurIPS'22 Workshop on Machine Learning in Structural Biology](https://www.mlsb.io/).

[[arxiv]](https://arxiv.org/abs/2212.09925) [[Github]](https://github.com/pemami4911/ppde) [[colab demo]](https://colab.research.google.com/drive/1s3heukQga1ShfxrAMRxNtZFfSwu_D_m7?usp=sharing)

{%
    include image.html
    img="/img/PPDE/Proteins_qual_1row.png"
    caption="The marginal frequency of each sequence position visualized on the 3D structure of the wild type is the fraction of protein variants in the sampled variant pool that have a mutation at that sequence position. For example, nearly 100% of discovered UBE4B variants have a mutation at position 1145."
%}

## Summary

Machine-learning-based directed evolution can be used to engineer new proteins that have improved productivity or catalyze new reactions. Basically, starting from a known protein sequence, directed evolution proceeds by identifying candidate mutations, experimentally verifying them, and then using the gained information to search for better mutations. [The intention is for this iterative process to resemble natural evolution](http://www.cheme.caltech.edu/groups/fha/old_website_2011_06_08/Arnold_ACR_1998.pdf). One way ML can help streamline this design loop is by optimizing the candidate indentification step.

In this paper, we introduce a plug and play protein sampler for this step. The sampler mixes and matches *unsupervised* evolutionary sequence models (e.g., protein language models) with *supervised* sequence models that map sequence to function, without needing any model re-training or fine-tuning. Sampling is performed in sequence space to keep things as ''black-box'' as possible. Plug and play samplers have been successful in other domains, such as for controlled [image](https://openaccess.thecvf.com/content_cvpr_2017/html/Nguyen_Plug__Play_CVPR_2017_paper.html) and [text](https://arxiv.org/abs/1912.02164) generation.

Searching through protein sequence space *efficiently*, which is high-dimensional and discrete, is notoriously difficult. We address this in our MCMC-based sampler by using the *gradient* of the (differentiable) target function to bias a mutation proposal distribution towards favorable mutations. We show that this approach outperforms classic random search.

{%
    include image.html
    img="/img/PPDE/main.gif"
%}

**Impact**: Improving the quality of candidate protein variants reduces the time spent conducting costly experiments in the wet lab and accelerates the engineering of new proteins.

## Checking intuitions with a toy binary MNIST experiment

The task is to evolve the MNIST digit on the right, initially a 1, to maximize the sum of the two digits *solely by flipping binary pixels*. Since the digit on the left is a 9, the initial sum of the digits is 10. The maximum is 18 (9 + 9).

Here's our sampler applied to a product of experts distribution that combines an unsupervised energy based model with a supervised ensemble of ConvNets trained to regress the sum.

{%
    include image.html
    img="/img/PPDE/PPDE_DAE_20fps.gif"
    caption=""
%}

If we remove the EBM from the product of experts, the search gets trapped in [adversarial optima](https://arxiv.org/abs/1312.6199)---white-noise images. This is because, in this case, gradient-based discrete MCMC is only using the gradient of the ConvNet ensemble to steer search. The EBM acts as a soft constraint that keeps search away from such optima.
{%
    include image.html
    img="/img/PPDE/PPDE_None_20fps.gif"
    caption=""
%}

If we replace gradient-based discrete MCMC with random-search-based simulated annealing, nearly every randomly sampled mutation is rejected as it does not increase the predicted sum.
{%
    include image.html
    img="/img/PPDE/simulated_annealing_20fps.gif"
%}

## In-silico directed evolution experiments

We conduct in-silico directed evolution experiments on 3 proteins with partially-characterized wide fitness landscapes (between 2-15 mutations from wild type): [PABP_YEAST](https://www.uniprot.org/uniprot/P04147), [UBE4B_MOUSE](https://www.uniprot.org/uniprotkb/Q9ES00/entry), and [GFP](https://www.uniprot.org/uniprotkb/P42212/entry).

Compared to random-walk-based black-box samplers (simulated annealing, Metropolis-adjusted Langevin Algorithm (MALA)), random sampling, and an evolutionary method (CMA-ES), our sampler explores sequence space most efficiently (higher $\log p(x)$ is better).

{%
    include image.html
    img="/img/PPDE/sampler_lineplots.png"
%}

We show that we can plug in different [protein language models](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v3): ESM2-35M, ESM2-150M, and ESM2-650M. We also combine ESM2-150M with a Potts model, which surprisingly achieved the best performance (red arrow).

{%
    include image.html
    img="/img/PPDE/PPDE_priors.png"
%}

See the paper for more details and results.