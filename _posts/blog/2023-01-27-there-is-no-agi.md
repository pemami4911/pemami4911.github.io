---
layout: post
title: "There is No Such Thing as AGI: An Evolutionary Perspective"
date: 2023-01-27
category: blog
byline: "Personal thoughts about AGI, evolution, and an articulation of my ethos as an AI researcher"
---

<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } },
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>

---
**Disclaimer**: This post contains my personal thoughts on artificial intelligence (AI) as of April 2023. I reserve the right to update my priors in light of new information as time passes. What is written here is mostly speculation and should be taken with a grain of salt.

## Haeckel's Tree

Let's look at where modern humans and machines are on Haeckel's evolutionary tree of life:

{%
  include image.html
  img="/img/AGI/Haeckel.png"
  caption="Tree of Vertebrates, from Ernst Haeckel's The Evolution of Man, fifth edition, London, 1910"
%}

The story here is that at some point, evolution pressured certain animals to grow large brains and walk upright.
Then, they started to develop and use tools (~2M years ago).
These tools further expanded their mental and physical faculties, which in turn helped make these creatures even smarter.
Being smarter meant better tools, which again made them smarter, and eventually they developed the digital computer.
Something unique about humans---perhaps the combination of our ability to reason about abstract concepts like recursion, large brains, dexterous hands, and a propensity to socialize in large communities---enables us to not only *use* tools to survive but also to vastly *improve* ourselves.

---

**Update April 2023**: There is evidence that certain non-primate organisms (e.g., crows) are also capable of [tool-use](https://www.scientificamerican.com/article/the-secret-lives-of-tool/) and [recursive sequence generation](https://www.science.org/doi/10.1126/sciadv.abq3356). The implication is that many things we previously believed were unique about human intellect are actually not so unique.

## There is no such thing as AGI

In pop culture, "artificial general intelligence" (AGI) is thought to be a sentient entity forged from silicon suddenly announced to the world in a press release by Big Tech Corp.
It is thought that this AGI will have far superior capabilities to humans and will likely be uncontrollable.
A Singularity follows when this AGI masters the ability to improve *itself*, without any human intervention.
I argue that there is not, and never will be, such an AGI.

## Powerful machines augment human intelligence

I believe this popular AGI fantasy ignores the effect that increasingly powerful technologies will have on humans and the role that humans have to play.
Rather, the situation we are in moreso resembles an evolving symbiosis of increasingly intelligent machines *and humans*.
We are constantly making newer, more powerful tools that further empower our own general intelligence---which I call *artificially augmented general intelligence* (AAGI).

For example, today I use a Google Doc to temporarily expand my short-term memory while sitting in on a brainstorming meeting.
We use satellites to augment our eyes to analyze the Earth's surface and microscopes to peer at things a nanometer long. 
We use Google Translate to speak to people in different languages, chat bots to help us write code faster, and supercomputers to forecast tomorrow's weather.
Imagine how an alien from another solar system might react if they were to see us driving a car.
They might think we have exoskeletons that we can take off and put on at will. 
We humans, or AAGIs, are constantly finding exciting ways to augment ourselves with artificial intelligences!

---

**Update April 2023**: I would like to highlight this phrase I use: "increasingly intelligent". It may seem that I am suggesting certain intelligences are greater/better/superior, and other (say, un-augmented) intelligences are lesser/worse/inferior. I do not believe this is a good perspective to have, and I wish to distance myself from it. Rather, I am content to pitch my philosophical tent in the camp of those who say there are different *kinds* of intelligences, and increasing the diversity of intelligences is a Good thing. In the future, I think we ought to stick to talking about specific capabilities that have been altered by technology (like I do in the previous paragraph) rather than using language that makes broad statements about intelligences. A related and important concern is that there is not yet equitable access to this technology in many parts of the world. This suggests a widening gap between the *haves* and *have nots*, which we should work to prevent.  

## Humans and machines as a single, cyber-physical organism

If we agree that powerful machines augment our intelligence, is it a stretch to say that soon we will consider *homo sapiens* and machines to be a *single cyber-physical* organism?

Let's imagine the point far along the tree of life where the line between what's human and and what's machine has blurred completely.
The AAGI cyber-physical organism that emerges at this point of convergence is likely totally inconceivable to us today.
It will not resemble AGI in the pop culture sense.
A quick glance back at the tree of life suggests this might take thousands or perhaps millions of years to happen, if it ever happens at all. 
Evolution hasn't really reacted yet to the invention of the steam engine.
It won't happen next year. Or in 10 years. Or 100. 

Others (including many scientists) are happy to call an embodied computational learning system with agency, capable of rapidly expanding its repertoire of real world skills, an "AGI". Would this represent a new bifurcation of the branch *homo sapiens* in our tree of life---a branch that has split off from our own? 
The crux of my perspective says that, in short answer, this is not the case.
I believe this would still be considered a *tool*, and we should not think about such tools outside of the context of their human designers and users.

---
**Update April 2023**: After initially writing this post, I came to learn about transhumanism, [its roots in eugenics](https://en.wikipedia.org/wiki/Julian_Huxley), and [posthumanism](https://en.wikipedia.org/wiki/Posthumanism). It was perhaps naive of me to suggest that an eventual convergence of homo sapiens and machines will be the result of random, ~undirected, natural evolution rather than an intentional goal being strived for by certain groups of people with ulterior motives. I came across Donna Haraway's Cyborg Theory, which offers an intersectional and feminist perspective on the convergence of homo sapiens and machines. I highly suggest reading the article [Transhumanism, Posthumanism, and the "Cyborg Identity"](https://dergipark.org.tr/en/download/article-file/1803280) for a summary of her manifesto in proper context. Here's an excerpt:
```
The “cyborg” of Haraway is a genderless and raceless mode of being imagined to find a way
towards equality by eliminating all sorts of problematic dualisms like that of self/other,
culture/nature, male/female, civilized/primitive, right/wrong, truth/illusion, total/partial,
God/man (Haraway 1990, 177). She intends to reach the so-called “posthuman state” of human beings,
namely “cyborg”, for finding a way to overcome various systems of domination.
```
However, perhaps unsurprisingly, some of Haraway's recent books have taken a [slight turn towards eugenics (population control, yuck!)](https://www.reddit.com/r/CriticalTheory/comments/pjt3ck/comment/hbyq1y0/?utm_source=share&utm_medium=web2x&context=3). It seems that, invariably, these theories all have a [modest proposal](https://en.wikipedia.org/wiki/A_Modest_Proposal) to make...

My key takeaway here is that *it is a mistake to try to optimize the evolution of homo sapiens as a solution for societal problems.* 

## Human knowledge in AI research
I don't think it is a hot take to say that out of the tired symbolists vs. connectionists debate, those arguing for the fusion of both paradigms were always right.
Just look at the recent success of [reinforcement learning from human feedback](https://openai.com/blog/instruction-following/), or RLHF, where human experts distill symbolic knowledge into large language models, making them significantly better.
Like our ancestors, we will continue to use our knowledge to build increasingly complex tools (e.g., learning systems), which we will in turn use to create even more powerful learning systems, *ad infinitum*.
There is a certain beauty to this cyclical, symbiotic relationship, at least to me.

---
**Update April 2023**: It has since been reported that [OpenAI exploited workers to obtain labeled data](https://time.com/6247678/openai-chatgpt-kenya-workers/), presumably for RLHF.  

## What are some implications of all this?

For one, we are already hurtling towards an energy consumption Singularity.
{%
    include image.html
    img="/img/AGI/energy.png"
%}

This also says something about [**AGI** Alignment/Safety](https://www.agisafetyfundamentals.com) (not to be conflated with **AI** Alignment/Safety).
If humans and machines are viewed as a single cyber-physical organism, the question of how to align the ethics of an alien AGI entity with our own makes no sense.
On the flip side, bad humans have always existed, so bad cyber-physical organisms have and will continue to exist.
This is nature. 
Perhaps evolution will eventually delete or re-write the parts of our DNA that predisposes some of us to do Evil things.
Another implication is that dedicating time and resources to building alien AGI entities today is highly questionable at best.
My belief is that we AI researchers should be making new tools that augment all people *today* to lead longer, happier, and healthier lives.

---
**Update April 2023**: Again, I believe the statement "*Perhaps evolution will eventually delete or re-write the parts of our DNA that predisposes some of us to do Evil things*" shows my initial naivety here. 
Although natural evolution is random and ~undirected, in reality, there are certain groups of people (e.g., those with eugenicist tendencies) who wish to control what traits are passed down over generations.
This is especially dangerous because "Good" and "Evil" are subjective concepts, and can be/have been weaponized for terrible things in the past.


## In conclusion
This was an attempt at articulating various thoughts I've been having about AI and evolution and how it relates to my ethos as an AI researcher. I claim to know very little about most of the topics in this post--which are also quite challenging to discuss, so by writing about them I hope to help myself achieve more clarity. 

Let's go out and build AI tools that *empower humans to lead better lives*.

