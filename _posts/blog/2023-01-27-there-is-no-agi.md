---
layout: post
title: "There Is No Such Thing as AGI: An Evolutionary Perspective"
date: 2023-01-27
category: blog
byline: "An articulation of my ethos as an AI researcher"
---

<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } },
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>

---
**Disclaimer**: This post contains my personal thoughts on artificial intelligence (AI) as of January 2023. I reserve the right to update my priors in light of new information as time passes. Anything written here should be taken with a grain of salt.

## Humans and machines as a single, cyber-physical organism

Let's consider modern humans and machines together on the evolutionary tree of life.

{%
  include image.html
  img="/img/AGI/Haeckel.png"
  caption="Tree of Vertebrates, from Ernst Haeckel's The Evolution of Man, fifth edition, London, 1910"
%}

Originally, evolution pressured certain animals to grow large brains and walk upright.
Then, they started to develop and use tools.
These tools further expanded their mental and physical faculties, which in turn helped make these creatures even smarter.
Being smarter meant better tools, which again made them smarter, and eventually they developed the digital computer.

If we see tools as an extension of our selves, is it really a stretch to view the *homo sapiens* branch of this tree as representing a *single cyber-physical* organism?

## There is no such thing as AGI
I argue that "artificial general intelligence" (AGI) is *not* some sentient entity forged from silicon that will be announced to the world one day in a press release by Big Tech Corp.
There is not and never will be such a thing.
Rather, what we have is a slowly evolving symbiosis of human and machine.

For example, today, I may use a Google Doc to temporarily expand my short-term memory while sitting in on a brainstorming meeting.
We use satellites to augment our eyes to analyze the Earth's surface and microscopes to peer at things a nanometer long. 
We use Google Translate to speak to people in different languages, chat bots to help us write code faster, and supercomputers to forecast tomorrow's weather.
Think about how an alien from another solar system might react if they were to see us driving a car.
They might think we have exoskeletons that we can take off and put on at will.Â 
 
Humans---generally intelligent beings---possess the ability to **augment** themselves with artificial intelligences! 
The implications of this are terribly exciting.

Now, maybe some of you are thinking, "well, AGI is just the point that humans and machines are slowly converging to, and it will be something totally incomprehensible to us today".
Maybe. It's plausible. 
But I would respond that this will take thousands or millions of years, if it ever happens at all. 
Evolution hasn't even reacted yet to the invention of the steam engine.
It will not happen next year. Or in 5 years. Or in 10 years.

## Human knowledge in AI research
I don't think it is a hot take to say that, out of that tired symbolists vs. connectionists debate, those arguing for the fusion of both paradigms were always right.
Just look at the recent success of [reinforcement learning from human feedback](https://openai.com/blog/instruction-following/), or RLHF, where human experts distill symbolic knowledge into large language models, making them significantly better.
Like our ancestors, we will continue to use our knowledge to build increasingly complex tools (e.g., learning systems), which we will in turn use to augment our faculty for creating more powerful learning systems, *ad infinitum*.
There is a certain beauty to this cyclical, symbiotic relationship, at least to me.


## What are the implications of all this?

For one, we are already hurtling towards a singularity of a kind.
{%
    include image.html
    img="/img/AGI/energy.png"
%}

This also says something about "AGI Alignment".
If humans and machines are viewed as a single cyber-physical organism slowly exploring an evolutionary landscape, it is not obvious that we need to worry about aligning the ethics of an alien AGI entity that will soon pop into existence.
Just like bad humans have existed, bad cyber-physical organisms will also exist.
This is nature. 
Maybe evolution will eventually delete or re-write the parts of our DNA that predisposes some of us to do Evil things.

Another implication is that it makes no sense to dedicate your time and resources towards building AGI today.
Natural evolution will run its course.
It is inevitable.
As AI researchers, we should instead strive to make new tools that augment *everyone* to help people *today* lead longer, happier, and healthier lives.

## In conclusion
This is my articulation of various thoughts I've been having about AI and evolution and how it relates to my ethos as an AI researcher. I hope it inspires others in this field, helps de-hype things a bit, and keeps us focused on a goal of *empowering humans to lead better lives*.

