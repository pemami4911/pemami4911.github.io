---
layout: post
title: "There is No Such Thing as AGI: An Evolutionary Perspective"
date: 2023-01-27
category: blog
byline: "Personal thoughts about AGI, evolution, and an articulation of my ethos as an AI researcher"
---

<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } },
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>

---
**Disclaimer**: This post contains my personal thoughts on artificial intelligence (AI) as of January 2023. I reserve the right to update my priors in light of new information as time passes. Anything written here should be taken with a grain of salt.

## Humans and machines as a single, cyber-physical organism

Let's consider modern humans and machines together on the evolutionary tree of life.

{%
  include image.html
  img="/img/AGI/Haeckel.png"
  caption="Tree of Vertebrates, from Ernst Haeckel's The Evolution of Man, fifth edition, London, 1910"
%}

Originally, evolution pressured certain animals to grow large brains and walk upright.
Then, they started to develop and use tools.
These tools further expanded their mental and physical faculties, which in turn helped make these creatures even smarter.
Being smarter meant better tools, which again made them smarter, and eventually they developed the digital computer.

If we see tools as an extension of our selves, is it really a stretch to view the *homo sapiens* branch of this tree as representing a *single cyber-physical* organism?

## There is no such thing as AGI
In pop culture, "artificial general intelligence" (AGI) is thought to be a sentient entity forged from silicon announced to the world in a press release by Big Tech Corp.
I argue that there is not and never will be such a thing.

Rather, I think the situation actually resembles a slowly evolving symbiosis of human and machine.
For example, today, I may use a Google Doc to temporarily expand my short-term memory while sitting in on a brainstorming meeting.
We use satellites to augment our eyes to analyze the Earth's surface and microscopes to peer at things a nanometer long. 
We use Google Translate to speak to people in different languages, chat bots to help us write code faster, and supercomputers to forecast tomorrow's weather.
Imagine how an alien from another solar system might react if they were to see us driving a car.
They might think we have exoskeletons that we can take off and put on at will.Â 
  
Humans---generally intelligent beings---possess the ability to **augment** themselves with artificial intelligences! 
The implications of this are terribly exciting.

Now, maybe some of you are thinking, "well, AGI will exist---it is the point to which humans and machines are slowly converging, and it will be something totally incomprehensible to us today".
Sure, I think this is plausible.
I would respond that this would take thousands or millions of years, if it ever happens at all. 
Evolution hasn't even reacted yet to the invention of the steam engine.
It won't not happen next year. Or in 5 years. Or in 10 years.

Many believe AGI will be an embodied computational learning system with agency, capable of rapidly expanding its repertoire of real world skills. 
Would this represent a true bifurcation of the branch *homo sapiens* in our tree of life---a new branch that has split off from our own? 
The crux of my perspective says that, in short answer, no.
We cannot think about such systems outside of the context of its human designers and users.
What we would have created is a powerful tool enabling *artificially augmented general intelligences* (AAGIs for short?)---the general intelligences in question being our own.

## Human knowledge in AI research
I don't think it is a hot take to say that, out of that tired symbolists vs. connectionists debate, those arguing for the fusion of both paradigms were always right.
Just look at the recent success of [reinforcement learning from human feedback](https://openai.com/blog/instruction-following/), or RLHF, where human experts distill symbolic knowledge into large language models, making them significantly better.
Like our ancestors, we will continue to use our knowledge to build increasingly complex tools (e.g., learning systems), which we will in turn use to augment our faculty for creating more powerful learning systems, *ad infinitum*.
There is a certain beauty to this cyclical, symbiotic relationship, at least to me.


## What are the implications of all this?

For one, we are already hurtling towards a singularity of a kind.
{%
    include image.html
    img="/img/AGI/energy.png"
%}

This also says something about [**AGI** Alignment/Safety](https://www.agisafetyfundamentals.com) (not to be conflated with **AI** Alignment/Safety).
If humans and machines are viewed as a single cyber-physical organism, the question of how to align the ethics of an alien AGI entity with our own makes no sense.
On the flip side, bad humans have always existed, so bad cyber-physical organisms have and will continue to exist.
This is nature. 
Perhaps evolution will eventually delete or re-write the parts of our DNA that predisposes some of us to do Evil things.
Another implication is that dedicating time and resources towards building AGI today is wasteful.

As AI researchers, my belief is that we should instead make new tools that augment all people *today* to lead longer, happier, and healthier lives.

## In conclusion
This is my articulation of various thoughts I've been having about AI and evolution and how it relates to my ethos as an AI researcher. I hope it inspires others in this field, helps de-hype things a bit, and keeps us focused on a goal of *empowering humans to lead better lives*.

