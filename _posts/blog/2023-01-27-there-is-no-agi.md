---
layout: post
title: "There is No Such Thing as AGI: An Evolutionary Perspective"
date: 2023-01-27
category: blog
byline: "Personal thoughts about AGI, evolution, and an articulation of my ethos as an AI researcher"
---

<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } },
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>

---
**Disclaimer**: This post contains my personal thoughts on artificial intelligence (AI) as of January 2023. I reserve the right to update my priors in light of new information as time passes. Anything written here should be taken with a grain of salt.

## Humans and machines as a single, cyber-physical organism

Let's consider modern humans and machines together on the evolutionary tree of life.

{%
  include image.html
  img="/img/AGI/Haeckel.png"
  caption="Tree of Vertebrates, from Ernst Haeckel's The Evolution of Man, fifth edition, London, 1910"
%}

Originally, evolution pressured certain animals to grow large brains and walk upright.
Then, they started to develop and use tools.
These tools further expanded their mental and physical faculties, which in turn helped make these creatures even smarter.
Being smarter meant better tools, which again made them smarter, and eventually they developed the digital computer.

If we see tools as an extension of our selves, is it a stretch to say that the *homo sapiens* branch of this tree represents a *single cyber-physical* organism?

## There is no such thing as AGI
In pop culture, "artificial general intelligence" (AGI) is thought to be a sentient entity forged from silicon suddenly announced to the world in a press release by Big Tech Corp.
It is thought that this AGI will have far superior capabilities to humans and will likely be uncontrollable.
I argue that there is not and never will be such a thing.

Rather, I think the situation we are in resembles a slowly evolving symbiosis of increasingly intelligent humans and machines.
We create powerful *tools* that further empower our *artificially augmented general intelligences* (AAGIs for short?)---the general intelligences in question being our own.

For example, today I use a Google Doc to temporarily expand my short-term memory while sitting in on a brainstorming meeting.
We use satellites to augment our eyes to analyze the Earth's surface and microscopes to peer at things a nanometer long. 
We use Google Translate to speak to people in different languages, chat bots to help us write code faster, and supercomputers to forecast tomorrow's weather.
Imagine how an alien from another solar system might react if they were to see us driving a car.
They might think we have exoskeletons that we can take off and put on at will.Â 
*Basically, we humans---generally intelligent beings---possess the ability to **augment** themselves with artificial intelligences!* 

Let's imagine the point far along the tree of life where humans and and machine might eventually converge at.
The cyber-physical organism that emerges at this point is likely totally inconceivable to us today.
It will not resemble AGI in the pop culture sense, and it will take thousands or perhaps millions of years for this to happen, if it ever happens at all. 
Evolution hasn't even reacted yet to the invention of the steam engine.
It won't happen next year. Or in 10 years. Or 100. 

Others are happy to call an embodied computational learning system with agency, capable of rapidly expanding its repertoire of real world skills, an "AGI". Would this represent a bifurcation of the branch *homo sapiens* in our tree of life---a new branch that has split off from our own? 
The crux of my perspective is that, in short answer, this is not the case.
I believe we should not think about such systems outside of the context of their human designers and users.

## Human knowledge in AI research
I don't think it is a hot take to say that out of the tired symbolists vs. connectionists debate, those arguing for the fusion of both paradigms were always right.
Just look at the recent success of [reinforcement learning from human feedback](https://openai.com/blog/instruction-following/), or RLHF, where human experts distill symbolic knowledge into large language models, making them significantly better.
Like our ancestors, we will continue to use our knowledge to build increasingly complex tools (e.g., learning systems), which we will in turn use to augment our faculty for creating more powerful learning systems, *ad infinitum*.
There is a certain beauty to this cyclical, symbiotic relationship, at least to me.


## What are the implications of all this?

For one, we are already hurtling towards a singularity of a kind.
{%
    include image.html
    img="/img/AGI/energy.png"
%}

This also says something about [**AGI** Alignment/Safety](https://www.agisafetyfundamentals.com) (not to be conflated with **AI** Alignment/Safety).
If humans and machines are viewed as a single cyber-physical organism, the question of how to align the ethics of an alien AGI entity with our own makes no sense.
On the flip side, bad humans have always existed, so bad cyber-physical organisms have and will continue to exist.
This is nature. 
Perhaps evolution will eventually delete or re-write the parts of our DNA that predisposes some of us to do Evil things.
Another implication is that dedicating time and resources to building alien AGI entities today is highly questionable at best.
My belief is that we AI researchers should be making new tools that augment all people *today* to lead longer, happier, and healthier lives.

## In conclusion
This is my articulation of various thoughts I've been having about AI and evolution and how it relates to my ethos as an AI researcher. I hope it inspires others in this field, helps de-hype things a bit, and keeps us focused on a goal of *empowering humans to lead better lives*.

